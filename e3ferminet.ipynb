{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a82b8824-c759-45d6-b7b6-f28d2d47d82b",
   "metadata": {
    "id": "a82b8824-c759-45d6-b7b6-f28d2d47d82b"
   },
   "source": [
    "# e3ferminet\n",
    "E(3)-equivariant neural network ansatz for atomic and molecular VMC calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WK1mXON5QK8-",
   "metadata": {
    "id": "WK1mXON5QK8-"
   },
   "source": [
    "#Background\n",
    "\n",
    "In computational chemistry, solving the ground state wavefunction of a molecule allows us to predict its properties accurately.\n",
    "However, an exact solution to the wavefunction is computationally infeasible for these quantum many-body systems,\n",
    "and various approximation methods have been proposed and studied over the last few decades.\n",
    "A classical method is the variational Monte Carlo method (VMC).\n",
    "VMC is a variant of the variational method, which identifies the ground state energy $E_0$ as the minimum of the energy functional\n",
    "\\begin{equation}\n",
    "    E[\\psi] = \\frac{\\langle \\psi| \\hat H |\\psi\\rangle}{\\langle \\psi | \\psi \\rangle} = \\frac{\\int dx\\, |\\psi(x)|^2 \\frac{\\hat H\\psi(x)}{\\psi(x)}}{\\int dx\\, |\\psi(x)|^2},\n",
    "\\end{equation}\n",
    "\n",
    "The value of the energy functional is lower-bounded by the ground state energy, so the variational method seeks to find better and better approximations for $E_0$ by guessing and refining an ansatz. The energy functional can then be computed by sampling points from $p(x) \\propto |\\psi(x)|^2$ to estimate the expectation value of $\\frac{\\hat H\\psi(x)}{\\psi(x)}$. The main difficulty remains in finding a good ansatz.\n",
    "\n",
    "One standard choice in VMC is to guess a wavefunction of the Slater-Jastrow type. This consists of a Slater determinant multiplied by a Jastrow factor (typically of the form $e^{J}$, with $J= \\sum_{ij} u_{ij}(|r_i - r_j|)$, i.e. a function of all pairwise distances to account for electron correlation. For example, the wavefunction ansatz PauliNet developed by Noe et al. uses a preliminary Hartree-Fock calculation as an input and expressiveness comes from the Jastrow factor and a backflow transformation, both represented as DNNs. However, one benefit of neural networks is that we don't need to be restricted to wavefunctions of this form, which have limitations such as being constrained to a finite basis set.\n",
    "\n",
    "In [this](https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.2.033429) paper, the authors used an ansatz parameterized by a neural network, and by performing gradient descent with $E[\\psi]$ as the loss function, they were able to accurately recover the ground state wavefunctions of a few small but challenging molecules. In this project, we will explore the potential of using an $E(3)$-equivariant neural network to parameterize the wavefunction. We will start by investigating single atoms. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7df8af5-7365-4955-95ce-86b98d16448f",
   "metadata": {
    "id": "c7df8af5-7365-4955-95ce-86b98d16448f",
    "tags": []
   },
   "source": [
    "## Single atom wavefunction\n",
    "Let's first study a single atom of atomic number $Z$. For $n$ electrons, the Hamiltonian is\n",
    "$$\n",
    "\\hat H = -\\frac{1}{2} \\sum_i \\nabla_i^2 - \\sum_i \\frac{Z}{r_i} + \\sum_{i<j} \\frac{1}{|\\mathbf r_i - \\mathbf r_j|}.\n",
    "$$\n",
    "We'll parameterize the multi-electron wavefunction $\\psi(\\mathbf r_1, \\ldots, \\mathbf r_n)$ with a neural network $\\phi_\\theta(\\mathbf r_1, \\ldots, \\mathbf r_n)$ that is $SO(3)$-equivariant in each input $\\mathbf r_i$, where we obtain $\\psi$ by antisymmetrizing $\\phi_\\theta$:\n",
    "$$\n",
    "\\psi(\\mathbf r_i, \\ldots, \\mathbf r_n) = \\sum_{\\sigma \\in S(n_\\uparrow) \\times S(n_\\downarrow)} \\mathrm{sgn}(\\sigma) \\phi_\\theta(\\mathbf r_{\\sigma(1)}, \\ldots, \\mathbf r_{\\sigma(n)}).\n",
    "$$\n",
    "The sum is over all products of permutations of $n_\\uparrow$ spin-up electrons with permutations of $n_\\downarrow$ spin-down electrons, where $n_\\uparrow + n_\\downarrow = n$, where WLOG we label the spin-up electrons as $1, \\ldots, n_\\uparrow$ and the spin-down electrons as $n_\\uparrow + 1, \\ldots, n$.\n",
    "\n",
    "For computing the expected energy in the state $\\psi$, we will for now use Monte Carlo integration since we don't yet have an efficient way of sampling from the distribution $p(X) \\propto |\\psi(X)|^2$. In other words, we sample many points $X_i = (\\mathbf r_{i1}, \\ldots, \\mathbf r_{in})$ where each $\\mathbf r_{ij}$ is chosen uniformly and independently from a ball of a certain radius $r_{max}$. We then approximate\n",
    "$$\n",
    "E[\\psi] \\approx \\frac{\\sum_i \\psi(X_i)^* \\hat H\\psi(X_i)}{\\sum_i \\psi(X_i)^* \\psi(X_i)}.\n",
    "$$\n",
    "To implement the boundary condition $\\psi \\to 0$ as $|X| \\to \\infty$, we will add a regularization term that penzalizes large values of $|\\psi|$ near $r = r_{\\max}$ by adding a regularization term $\\lambda \\sum_i |\\frac{r_i}{r_{max}}|^\\beta$ for some large exponent $\\beta$ to the Hamiltonian.\n",
    "\n",
    "Finally, we perform gradient descent with\n",
    "$$\n",
    "\\theta \\gets \\theta - \\alpha \\nabla_\\theta E[\\psi]\n",
    "$$\n",
    "to find the ground state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ssS1N5EK1YVr",
   "metadata": {
    "executionInfo": {
     "elapsed": 6685,
     "status": "ok",
     "timestamp": 1683157607507,
     "user": {
      "displayName": "Zhening Li",
      "userId": "09190340313207387590"
     },
     "user_tz": 240
    },
    "id": "ssS1N5EK1YVr"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install e3nn-jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "HVR3g9uX1lNU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2130,
     "status": "ok",
     "timestamp": 1683157613241,
     "user": {
      "displayName": "Zhening Li",
      "userId": "09190340313207387590"
     },
     "user_tz": 240
    },
    "id": "HVR3g9uX1lNU",
    "outputId": "709b7844-582d-4f85-c1b1-9bb447861eba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.8\n",
      "0.6.9\n",
      "0.1.5\n",
      "0.17.4\n",
      "TFRT_CPU_0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, jacobian\n",
    "import flax\n",
    "import optax\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "\n",
    "import e3nn_jax as e3nn  # import e3nn-jax\n",
    "\n",
    "jnp.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(jax.__version__)\n",
    "print(flax.__version__)\n",
    "print(optax.__version__)\n",
    "print(e3nn.__version__)\n",
    "print(jnp.ones(()).device())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8DCl1_flg-bB",
   "metadata": {
    "id": "8DCl1_flg-bB"
   },
   "source": [
    "### Hydrogen\n",
    "\n",
    "As a proof of concept, we'll first try predicting the hydrogen atom ground state wavefunction. We'll use a simple MLP with a single hidden layer with 5 neurons, where the input is $|\\mathbf r|^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82gyK7TihtA",
   "metadata": {
    "id": "f82gyK7TihtA"
   },
   "source": [
    "### Helium\n",
    "\n",
    "Now, let's try the helium atom, which is the most electrons we can have before we have to introduce antisymmetrization. We'll use a simple MLP with two hidden layers with 5 neurons each, where the inputs to the MLP are scalars $|\\mathbf r_1|^2$, $|\\mathbf r_2|^2$, and $\\mathbf r_1 \\cdot \\mathbf r_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "_D2LoiEtooBQ",
   "metadata": {
    "executionInfo": {
     "elapsed": 3299,
     "status": "ok",
     "timestamp": 1683160517573,
     "user": {
      "displayName": "Zhening Li",
      "userId": "09190340313207387590"
     },
     "user_tz": 240
    },
    "id": "_D2LoiEtooBQ"
   },
   "outputs": [],
   "source": [
    "class ToyAnsatz:\n",
    "    def __init__(self, Z, N_up, N_down, config):\n",
    "        self.Z = Z\n",
    "        self.N_up = N_up\n",
    "        self.N_down = N_down\n",
    "\n",
    "        self.mlp = e3nn.flax.MultiLayerPerceptron([5, 5, 5, 1], act=jax.nn.gelu, output_activation=jax.nn.sigmoid)\n",
    "        self.envelope = lambda zeta, coords: jnp.exp(-zeta * jnp.linalg.norm(coords, axis=-1))\n",
    "\n",
    "        @jit\n",
    "        def wavefunction(w, coords):  # coords can be unbatched or batched\n",
    "            # TO-DO antisymmetrize the wavefunction at the end (spin-up and spin-down separately)\n",
    "            x = e3nn.tensor_square(e3nn.IrrepsArray(f\"{self.Z}x1o\", coords)).filter(keep=\"0e\")\n",
    "            return self.mlp.apply(w[\"mlp\"], x.array).squeeze(-1) * self.envelope(jnp.abs(w[\"envelope\"]), coords)\n",
    "        self.wavefunction = wavefunction\n",
    "\n",
    "    def init_weights(self, random_key):  # coords can be batched or unbatched\n",
    "        subkey1, subkey2 = jax.random.split(random_key)\n",
    "        coords = jnp.empty((3 * self.Z,))\n",
    "        x = e3nn.tensor_square(e3nn.IrrepsArray(f\"{self.Z}x1o\", coords)).filter(keep=\"0e\")\n",
    "        return {\n",
    "            \"mlp\": self.mlp.init(subkey1, x),\n",
    "            \"envelope\": jnp.sqrt(jax.random.chisquare(subkey2, df=2)) * self.Z\n",
    "        }\n",
    "\n",
    "class ManualAnsatz:\n",
    "    def __init__(self, Z, N_up, N_down, config):\n",
    "        self.Z = Z\n",
    "        self.N_up = N_up\n",
    "        self.N_down = N_down\n",
    "        self.N = N_up + N_down\n",
    "\n",
    "        self.hidden_irreps = \"5x0e+5x1e+5x1o+5x2e+5x2o\"\n",
    "        self.hidden_irreps_before_gate = \"25x0e+5x1e+5x1o+5x2e+5x2o\"\n",
    "        self.lmax = 2\n",
    "        # assert self.lmax + 1 == len(self.hidden_channels)\n",
    "\n",
    "        self.tensor = lambda input: e3nn.tensor_square(input).filter(keep=e3nn.Irrep.iterator(lmax=self.lmax))\n",
    "        self.linear1 = e3nn.flax.Linear(irreps_out=self.hidden_irreps_before_gate, biases=True)\n",
    "        self.linear2 = e3nn.flax.Linear(irreps_out=self.hidden_irreps_before_gate, biases=True)\n",
    "        self.linear_head = e3nn.flax.Linear(irreps_out=\"0e\", biases=True)\n",
    "        self.envelope = lambda zeta, coords: jnp.exp(-zeta * jnp.linalg.norm(coords, axis=-1))\n",
    "\n",
    "        @jit\n",
    "        def wavefunction(w, coords):  # coords can be unbatched or batched\n",
    "            coords_irreps = e3nn.IrrepsArray(f\"{self.N}x1o\", coords)\n",
    "            x = self.tensor(coords_irreps)\n",
    "            x = self.linear1.apply(w[\"linear1\"], x)\n",
    "            x = e3nn.gate(x)\n",
    "            x = self.linear2.apply(w[\"linear2\"], x)\n",
    "            x = e3nn.gate(x)\n",
    "            x = x.filter(keep=\"0e\")\n",
    "            x = self.linear_head.apply(w[\"linear_head\"], x).array.squeeze(-1)\n",
    "            return x * self.envelope(jnp.abs(w[\"envelope\"]), coords)\n",
    "        self.wavefunction = wavefunction\n",
    "\n",
    "    def init_weights(self, random_key):  # coords can be batched or unbatched\n",
    "        w = {}\n",
    "        subkey1, subkey2, subkey3, subkey4 = jax.random.split(random_key, num=4)\n",
    "        coords = jnp.empty((3 * self.Z,))\n",
    "        coords_irreps = e3nn.IrrepsArray(f\"{self.N}x1o\", coords)\n",
    "        x = self.tensor(coords_irreps)\n",
    "        w[\"linear1\"] = self.linear1.init(subkey1, x)\n",
    "        x = self.linear1.apply(w[\"linear1\"], x)\n",
    "        print(x.irreps)\n",
    "        x = e3nn.gate(x)\n",
    "        w[\"linear2\"] = self.linear2.init(subkey2, x)\n",
    "        x = self.linear2.apply(w[\"linear2\"], x)\n",
    "        x = e3nn.gate(x)\n",
    "        x = x.filter(keep=\"0e\")\n",
    "        w[\"linear_head\"] = self.linear_head.init(subkey3, x)\n",
    "        x = self.linear_head.apply(w[\"linear_head\"], x).array.squeeze(-1)\n",
    "        w[\"envelope\"] = jnp.sqrt(jax.random.chisquare(subkey4, df=2)) * self.Z\n",
    "        return w\n",
    "\n",
    "class FerminetAnsatz:\n",
    "    def __init__(self, Z, N_up, N_down, config):\n",
    "        self.Z = Z\n",
    "        self.N_up = N_up\n",
    "        self.N_down = N_down\n",
    "        self.N = N_up + N_down\n",
    "\n",
    "        # self.hidden_irreps = \"5x0e+5x1e+5x1o+5x2e+5x2o\"\n",
    "        # self.hidden_irreps_before_gate = \"25x0e+5x1e+5x1o+5x2e+5x2o\"\n",
    "        self.lmax = config.get(\"lmax\", )\n",
    "\n",
    "        self.tensor = lambda input: e3nn.tensor_square(input).filter(keep=e3nn.Irrep.iterator(lmax=self.lmax))\n",
    "        self.linear1 = e3nn.flax.Linear(irreps_out=self.hidden_irreps_before_gate, biases=True)\n",
    "        self.linear2 = e3nn.flax.Linear(irreps_out=self.hidden_irreps_before_gate, biases=True)\n",
    "        self.linear_head = e3nn.flax.Linear(irreps_out=\"0e\", biases=True)\n",
    "        self.envelope = lambda zeta, coords: jnp.exp(-zeta * jnp.linalg.norm(coords, axis=-1))\n",
    "\n",
    "        @jit\n",
    "        def wavefunction(w, coords):  # coords can be unbatched or batched\n",
    "            coords_irreps = e3nn.IrrepsArray(f\"{self.N}x1o\", coords)\n",
    "            x = self.tensor(coords_irreps)\n",
    "            x = self.linear1.apply(w[\"linear1\"], x)\n",
    "            x = e3nn.gate(x)\n",
    "            x = self.linear2.apply(w[\"linear2\"], x)\n",
    "            x = e3nn.gate(x)\n",
    "            x = x.filter(keep=\"0e\")\n",
    "            x = self.linear_head.apply(w[\"linear_head\"], x).array.squeeze(-1)\n",
    "            return x * self.envelope(jnp.abs(w[\"envelope\"]), coords)\n",
    "        self.wavefunction = wavefunction\n",
    "\n",
    "    def init_weights(self, random_key):  # coords can be batched or unbatched\n",
    "        w = {}\n",
    "        subkey1, subkey2, subkey3, subkey4 = jax.random.split(random_key, num=4)\n",
    "        coords = jnp.empty((3 * self.Z,))\n",
    "        coords_irreps = e3nn.IrrepsArray(f\"{self.N}x1o\", coords)\n",
    "        x = self.tensor(coords_irreps)\n",
    "        w[\"linear1\"] = self.linear1.init(subkey1, x)\n",
    "        x = self.linear1.apply(w[\"linear1\"], x)\n",
    "        print(x.irreps)\n",
    "        x = e3nn.gate(x)\n",
    "        w[\"linear2\"] = self.linear2.init(subkey2, x)\n",
    "        x = self.linear2.apply(w[\"linear2\"], x)\n",
    "        x = e3nn.gate(x)\n",
    "        x = x.filter(keep=\"0e\")\n",
    "        w[\"linear_head\"] = self.linear_head.init(subkey3, x)\n",
    "        x = self.linear_head.apply(w[\"linear_head\"], x).array.squeeze(-1)\n",
    "        w[\"envelope\"] = jnp.sqrt(jax.random.chisquare(subkey4, df=2)) * self.Z\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8xhMN-w4dYY3",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1683160534949,
     "user": {
      "displayName": "Zhening Li",
      "userId": "09190340313207387590"
     },
     "user_tz": 240
    },
    "id": "8xhMN-w4dYY3"
   },
   "outputs": [],
   "source": [
    "class E3FerminetAtom:\n",
    "    def __init__(self, config):\n",
    "        self.Z = config.get(\"Z\", 1)\n",
    "        self.N_up = config.get(\"N_up\", 1)\n",
    "        self.N_down = config.get(\"N_down\", 1)\n",
    "        self.sampler = config.get(\"sampler\")  # use M-H if None\n",
    "        self.sampling_dist = config.get(\"sampling_dist\")  # use M-H if None\n",
    "        assert((self.sampler is None) == (self.sampling_dist is None))\n",
    "        self.N_samples = config.get(\"batch_size\", 20000)\n",
    "        self.num_batches = config.get(\"num_batches\", 1000)\n",
    "        self.lr = config.get(\"lr\", 0.1)\n",
    "        self.validate_every = config.get(\"validate_every\", 2000)\n",
    "        self.moving_avg_coeff = config.get(\"moving_avg_coeff\", 0.1)\n",
    "        self.regularize = \"regularize\" in config\n",
    "        self.regularize_pow = config[\"regularize\"].get(\"pow\", 8) if self.regularize else None\n",
    "        self.regularize_coeff = config[\"regularize\"].get(\"coeff\", 100) if self.regularize else None\n",
    "        self.regularize_max_r = config[\"regularize\"].get(\"max_r\", 2) if self.regularize else None\n",
    "        self.patience = config.get(\"patience\", 200)\n",
    "        self.random_key = jax.random.PRNGKey(config.get(\"random_seed\", 0))\n",
    "\n",
    "        self.ansatz = ManualAnsatz(self.Z, self.N_up, self.N_down, config[\"ansatz\"])\n",
    "\n",
    "        self.w = None\n",
    "        self.w_list = None\n",
    "        self.energy_moving_avgs = None\n",
    "\n",
    "        @jit\n",
    "        def local_kinetic_energy(w, coords):  # coords must be unbatched\n",
    "            def laplacian(coords):\n",
    "                return jnp.einsum('ii->', jacobian(jacobian(self.ansatz.wavefunction, argnums=1), argnums=1)(w, coords))\n",
    "            return -0.5 * laplacian(coords) / self.ansatz.wavefunction(w, coords)\n",
    "        self._local_kinetic_energy = local_kinetic_energy\n",
    "\n",
    "        @jit\n",
    "        def local_potential_energy(w, coords):  # coords must be unbatched\n",
    "            coords = coords.reshape((-1, 3))\n",
    "            V_e_p = -self.Z * jnp.sum(1 / jnp.linalg.norm(coords, axis=1), axis=0)\n",
    "            relative_dists = jnp.linalg.norm(jnp.expand_dims(coords, axis=0) - jnp.expand_dims(coords, axis=1), axis=2)\n",
    "            V_e_e = jnp.sum(1 / jnp.where(relative_dists == 0, np.inf, relative_dists)) / 2\n",
    "            return V_e_p + V_e_e\n",
    "        self._local_potential_energy = local_potential_energy\n",
    "\n",
    "        @jit\n",
    "        def local_energy(w, coords):  # coords must be unbatched\n",
    "            return local_kinetic_energy(w, coords) + local_potential_energy(w, coords)\n",
    "        self._local_energy = local_energy\n",
    "\n",
    "        @jit\n",
    "        def energy(w, coords_batch):\n",
    "            # If sampling_dist is None, assume sampling from wavefunction\n",
    "            local_energies = vmap(local_energy, in_axes=(None, 0))(w, coords_batch)\n",
    "            if self.sampling_dist is None:\n",
    "                return jnp.mean(local_energies)\n",
    "            psi = self.ansatz.wavefunction(w, coords_batch)\n",
    "            scaled_probs = psi ** 2 / vmap(self.sampling_dist)(coords_batch)\n",
    "            return jnp.dot(scaled_probs, local_energies) / jnp.sum(scaled_probs)\n",
    "        self._energy = energy\n",
    "\n",
    "        if self.regularize:\n",
    "            @jit\n",
    "            def regularized_energy(w, coords_batch):\n",
    "                reshaped_coords_batch = coords_batch.reshape((coords_batch.shape[0], -1, 3))\n",
    "                penalty = jnp.sum((jnp.linalg.norm(reshaped_coords_batch, axis=2) / self.regularize_max_r) ** self.regularize_pow, axis=1)\n",
    "                if self.sampling_dist is None:\n",
    "                    cum_penalty = jnp.mean(penalty)\n",
    "                else:\n",
    "                    psi = self.ansatz.wavefunction(w, coords_batch)\n",
    "                    scaled_probs = psi ** 2 / vmap(self.sampling_dist)(coords_batch)\n",
    "                    cum_penalty = jnp.dot(scaled_probs, penalty) / jnp.sum(scaled_probs)\n",
    "                return energy(w, coords_batch) + self.regularize_coeff * cum_penalty\n",
    "            self._regularized_energy = regularized_energy\n",
    "        \n",
    "        if self.sampler is None:\n",
    "            self.MH_stdev = config[\"MH\"].get(\"stdev\", 0.2)\n",
    "            self.MH_warmup = config[\"MH\"].get(\"warmup\", 500)\n",
    "            self.MH_interval = config[\"MH\"].get(\"interval\", 10)\n",
    "            self.MH_batch_size = config[\"MH\"].get(\"batch_size\", 64)\n",
    "            self.sampled_coords = None\n",
    "            def sampler(random_key, Z, num_samples):\n",
    "                # returns jnp array of shape (num_samples, 3*Z) sampled from the wavefunction\n",
    "                if self.sampled_coords is None:\n",
    "                    warmup = self.MH_warmup\n",
    "                    random_key, subkey = jax.random.split(random_key)\n",
    "                    self.sampled_coords = self.MH_stdev * jax.random.normal(subkey, (self.MH_batch_size, 3*Z))\n",
    "                else:\n",
    "                    warmup = self.MH_interval\n",
    "                coords = []\n",
    "                num_iters = warmup + (num_samples - 1) // self.MH_batch_size + 1\n",
    "                num_coords_remaining = num_samples\n",
    "                for i in range(num_iters):\n",
    "                    random_key, subkey = jax.random.split(random_key)\n",
    "                    proposal_coords = self.sampled_coords + self.MH_stdev * jax.random.normal(subkey, (self.MH_batch_size, 3*Z))\n",
    "                    acceptance_ratios = (self.ansatz.wavefunction(self.w, proposal_coords) / self.ansatz.wavefunction(self.w, self.sampled_coords)) ** 2\n",
    "                    random_key, subkey = jax.random.split(random_key)\n",
    "                    self.sampled_coords = jnp.where(np.expand_dims(jax.random.uniform(subkey, (self.MH_batch_size,)) < acceptance_ratios, axis=1),\n",
    "                                                    proposal_coords,\n",
    "                                                    self.sampled_coords)\n",
    "                    if i >= warmup:\n",
    "                        if self.MH_batch_size <= num_coords_remaining:\n",
    "                            coords_to_add = self.sampled_coords\n",
    "                            num_coords_remaining -= self.MH_batch_size\n",
    "                        else:\n",
    "                            coords_to_add = self.sampled_coords[:num_coords_remaining]\n",
    "                            num_coords_remaining = 0\n",
    "                        coords.append(coords_to_add)\n",
    "                return jnp.concatenate(coords)\n",
    "                # self.sampled_coords = jax.random.normal(random_key, (3*Z,))\n",
    "                # for i in range(500): #need time to stabilize chain, tune this number\n",
    "                #     random_key, subkey = jax.random.split(random_key)\n",
    "                #     proposal_coords = self.sampled_coords + 0.1*jax.random.normal(subkey, (3*Z,)) #need to tune stdev\n",
    "                #     a = self.ansatz.wavefunction(self.w, proposal_coords)**2 / self.ansatz.wavefunction(self.w, x_init)**2\n",
    "                #     if jax.random.uniform(subkey) < a:\n",
    "                #         self.sample_coords = proposal_coords\n",
    "                # coords = []\n",
    "                # for i in range(num_samples):\n",
    "                #     random_key, subkey = jax.random.split(random_key)\n",
    "                #     proposal_coords = x_init + 0.3*jax.random.normal(subkey, (3*Z,))\n",
    "                #     a = self.ansatz.wavefunction(self.w, proposal_coords)**2 / self.ansatz.wavefunction(self.w, x_init)**2\n",
    "                #     if jax.random.uniform(subkey) < a:\n",
    "                #         x_init = proposal_coords\n",
    "                #     coords.append(x_init)\n",
    "                # return jax.numpy.array(coords)\n",
    "            self.sampler = sampler\n",
    "        else:\n",
    "            assert \"MH\" not in config\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.random_key, subkey = jax.random.split(self.random_key)\n",
    "        self.w = self.ansatz.init_weights(subkey)\n",
    "        print(\"WEIGHTS:\", self.w)\n",
    "        # print(\"ENERGY:\", self._energy(self.w, coords_batch))\n",
    "        # if self.regularize:\n",
    "        #     print(\"REGULARIZED ENERGY:\", self._regularized_energy(self.w, coords_batch))\n",
    "    \n",
    "    def train_loop(self):\n",
    "        # Training loop\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        grad_energy = jit(grad(self._regularized_energy)) if self.regularize else jit(grad(self._energy))\n",
    "\n",
    "        optimizer = optax.adamw(learning_rate=self.lr)\n",
    "        opt_state = optimizer.init(self.w)\n",
    "\n",
    "        weights = [self.w]\n",
    "        self.random_key, subkey = jax.random.split(self.random_key)\n",
    "        coords_batch = self.sampler(subkey, self.Z, self.N_samples)\n",
    "        energy = self._energy(self.w, coords_batch)\n",
    "        energies = [energy]\n",
    "        energy_moving_avgs = [energy]\n",
    "        for step in tqdm(range(self.num_batches)):\n",
    "            self.random_key, subkey = jax.random.split(self.random_key)\n",
    "            coords_batch = self.sampler(subkey, self.Z, self.N_samples)\n",
    "            grads = grad_energy(self.w, coords_batch)\n",
    "            updates, opt_state = optimizer.update(grads, opt_state, self.w)\n",
    "            self.w = optax.apply_updates(self.w, updates)\n",
    "            weights.append(self.w)\n",
    "            energy = self._energy(self.w, coords_batch)\n",
    "            energies.append(energy)\n",
    "            energy_moving_avgs.append(energy_moving_avgs[-1] * (1 - self.moving_avg_coeff) + energy * self.moving_avg_coeff)\n",
    "            if step % self.validate_every == 0:\n",
    "                self.test()\n",
    "            if self.patience is not None and step - np.argmin(energy_moving_avgs) >= self.patience:\n",
    "                break\n",
    "        self.w_list = weights\n",
    "        self.energy_moving_avgs = energy_moving_avgs\n",
    "        learning_curve_df = pd.DataFrame({\"Batch index\": np.arange(len(energy_moving_avgs)), \"Energy\": energies})\n",
    "        fig = px.line(learning_curve_df, x=\"Batch index\", y=\"Energy\")\n",
    "        fig.show()\n",
    "    \n",
    "    def choose_weights(self, idx):\n",
    "        if idx == \"best\":\n",
    "            idx = jnp.argmin(self.energy_moving_avgs)\n",
    "            print(f\"BEST INDEX: {idx}\")\n",
    "        elif idx == \"last\":\n",
    "            idx = -1\n",
    "        self.w = self.w_list[idx]\n",
    "\n",
    "    def test(self, test_N_samples=50000):\n",
    "        self.random_key, subkey = jax.random.split(self.random_key)\n",
    "        coords_batch = self.sampler(subkey, self.Z, test_N_samples)\n",
    "        print(\"GROUND STATE ENERGY: {:.4f}\".format(self._energy(self.w, coords_batch)))\n",
    "    \n",
    "    def plot_one_electron_radial(self, max_r, plot_samples=5000):\n",
    "        radii = jnp.linspace(0, max_r, plot_samples+1)\n",
    "        coords_batch = np.hstack((np.expand_dims(radii, axis=1), np.zeros((plot_samples+1, 3*self.Z - 1))))\n",
    "        psi = self.ansatz.wavefunction(self.w, coords_batch)\n",
    "        x_label = \"$r$\"\n",
    "        y_label = \"$\\\\psi(r\\\\hat e_z, 0, \\\\ldots, 0)$\"\n",
    "        df = pd.DataFrame({x_label: radii, y_label: psi})\n",
    "        fig = px.line(df, x=x_label, y=y_label)\n",
    "        fig.show()\n",
    "\n",
    "    def plot_density_3D(self, plot_samples=5000):\n",
    "        self.random_key, subkey = jax.random.split(self.random_key)\n",
    "        coords_batch = self.sampler(subkey, self.Z, plot_samples)\n",
    "        densities = vmap(self.ansatz.wavefunction)(self.w, coords_batch) ** 2\n",
    "        max_density = jnp.max(densities)\n",
    "        self.random_key, subkey = jax.random.split(self.random_key)\n",
    "        coords_batch = coords_batch[max_density * jax.random.uniform(subkey, shape=(plot_samples,)) < densities]\n",
    "        df = pd.DataFrame(coords_batch.reshape((-1, 3)), columns=['x', 'y', 'z'])\n",
    "        print(df.head())\n",
    "        fig = px.scatter_3d(df, x='x', y='y', z='z')\n",
    "        fig.show()\n",
    "    \n",
    "    def plot_density_2D(self, pixel_size=0.01, step_size=0.1):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "HEdsDKzfmISU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1ylC_Thr20Cnz53nPZK5L-O1wsj0ZiaCR"
    },
    "executionInfo": {
     "elapsed": 5083654,
     "status": "ok",
     "timestamp": 1683165638579,
     "user": {
      "displayName": "Zhening Li",
      "userId": "09190340313207387590"
     },
     "user_tz": 240
    },
    "id": "HEdsDKzfmISU",
    "outputId": "9bd5e7a6-6d09-4224-88e1-ba63c6d5a4c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_r_hydrogen = 4\n",
    "hydrogen_config = {\n",
    "    \"random_seed\": 1,\n",
    "    \"Z\": 1,\n",
    "    \"N_up\": 1,\n",
    "    \"N_down\": 0,\n",
    "    # \"batch_size\": 2000,\n",
    "    # \"num_batches\": 50,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_batches\": 1000,\n",
    "    \"patience\": None,\n",
    "    # \"lr\": 0.001,\n",
    "    \"lr\": 1e-5,\n",
    "    #\"sampling_dist\": lambda coords: 1,\n",
    "    #\"sampler\": lambda random_key, Z, num_samples: jax.random.ball(random_key, 3, shape=(num_samples, Z)).reshape((num_samples, -1)) * max_r_hydrogen,\n",
    "    \"sampling_dist\" : None,\n",
    "    \"sampler\" : None,\n",
    "    \"moving_avg_coeff\": 0.1,\n",
    "    \"ansatz\": {},\n",
    "    \"MH\": {\n",
    "        \"stdev\": 0.2,\n",
    "        \"warmup\": 500,\n",
    "        \"batch_size\": 64\n",
    "    }\n",
    "    # \"regularize\": {\n",
    "    #     \"max_r\": max_r_hydrogen,\n",
    "    #     \"pow\": 8,\n",
    "    #     \"coeff\": 1\n",
    "    # }\n",
    "}\n",
    "\n",
    "max_r_helium = 2\n",
    "helium_config = {\n",
    "    \"random_seed\": 0,\n",
    "    \"Z\": 2,\n",
    "    \"N_up\": 1,\n",
    "    \"N_down\": 1,\n",
    "    # \"batch_size\": 2000,\n",
    "    # \"num_batches\": 25,\n",
    "    \"batch_size\": 128,\n",
    "    \"num_batches\": 100000,\n",
    "    \"validate_every\": 2000,\n",
    "    \"patience\": None,\n",
    "    \"lr\": optax.warmup_cosine_decay_schedule(5e-5, 5e-4, 100, 100000, end_value=5e-6, exponent=1.0),\n",
    "    # \"sampling_dist\": lambda coords: 1,\n",
    "    # \"sampler\": lambda random_key, Z, num_samples: jax.random.ball(random_key, 3, shape=(num_samples, Z)).reshape((num_samples, -1)) * max_r_helium,\n",
    "    \"sampling_dist\" : None,\n",
    "    \"sampler\" : None,\n",
    "    \"ansatz\": {},\n",
    "    \"MH\": {\n",
    "        \"stdev\": 0.2,\n",
    "        \"warmup\": 500,\n",
    "        \"interval\": 10,\n",
    "        \"batch_size\": 64\n",
    "    }\n",
    "    # \"regularize\": {\n",
    "    #     \"max_r\": max_r_helium,\n",
    "    #     \"regularize_pow\": 8,\n",
    "    #     \"regularize_coeff\": 0,\n",
    "    # }\n",
    "}\n",
    "\n",
    "atom_model = E3FerminetAtom(helium_config)\n",
    "atom_model.train_loop()\n",
    "atom_model.choose_weights(\"last\")\n",
    "atom_model.test()\n",
    "atom_model.plot_one_electron_radial(4)\n",
    "# atom_model.plot_density_3D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "RuNSWccOj2PR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 760,
     "status": "ok",
     "timestamp": 1683150554180,
     "user": {
      "displayName": "Zhening Li",
      "userId": "09190340313207387590"
     },
     "user_tz": 240
    },
    "id": "RuNSWccOj2PR",
    "outputId": "090cd5eb-022c-48b6-8d6f-2a3ef12bf3d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5x0e+5x1e+5x1o+5x2e+5x2o"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e3nn.gate(\"25x0e+0x0o+5x1e+5x1o+5x2e+5x2o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CRW5468gZcPH",
   "metadata": {
    "id": "CRW5468gZcPH"
   },
   "source": [
    "#Importance Sampling\n",
    "\n",
    "One source of inaccuracy in the Monte Carlo integration is that we were sampling from a uniform ball. But this over-prioritizes points far away from the origin, and points near the nucleus don't get sampled enough. This creates a large variance in the integral. We want to try importance sampling, where we sample from a known distribution and calculate the Hamiltonian at those points. Then each sample gets weighted. This works as follows:\n",
    "\n",
    "Suppose we are trying to sample the energy $E(x)$ from a distribution $p(x)$, and we want to calculate\n",
    "$$\\langle E \\rangle = \\int E(x)p(x)dx$$\n",
    "but $p(x)$ is difficult to sample from (or may not even be normalized!). We pick a sampling distribution $q(x)$, and calculate\n",
    "$$\\int E(x)p(x)dx=\\int E(x)q(x)\\frac{p(x)}{q(x)}dx \\approx \\sum_i E(x_i) \\frac{p(x)}{q(x)}$$\n",
    "where the points $x_i$ are sampled from $q$. In the case of the hydrogen atom, the radial distribution function is close enough to a $\\chi$-squared distribution, so we use that as a sampling distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_xDouNhNSQc7",
   "metadata": {
    "id": "_xDouNhNSQc7"
   },
   "source": [
    "Although decently accurate for hydrogen, there's still some distance to the true ground state energy for helium! Here are some possible approaches for improving our accuracy:\n",
    "- More expressive neural network, e.g. including biases, more layers, more neurons...\n",
    "- More efficient and accurate sampling\n",
    "- Better way to impose wavefunction boundary conditions\n",
    "\n",
    "We have only been using real-valued wavefunctions so far, which is a big limitation as well. So we should probably allow complex-valued wavefunctions sometime soon.\n",
    "\n",
    "The [paper](https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.2.033429) we're referencing has some quite sophisticated techniques for wavefunction parameterization and optimization, and we're considering incorporating some of them into our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HRsHGpq6ux_A",
   "metadata": {
    "id": "HRsHGpq6ux_A"
   },
   "source": [
    "# NEXT STEPS\n",
    "\n",
    "Zed: Continue with current approach\n",
    "- Refactor\n",
    "- Implement MH and importance sampling\n",
    "- Implement envelope\n",
    "\n",
    "Alec: Equivariant basis function approach\n",
    "\n",
    "Input $\\mathbf R_A$ (positions of all nuclei) (or relative distances?).\n",
    "Neural network outputs coefficients $c_i$ so that output wavefunction is\n",
    "$$\n",
    "\\psi = \\sum_i c_i \\psi_i\n",
    "$$\n",
    "Approach 1:\n",
    "Each electron has a molecular orbital $$\\phi_i$$ written as a linear combination of equivariant basis functions\n",
    "$$\n",
    "\\phi_i = \\sum_j c_{ij}{}^l_m B_j{}^l_m\n",
    "$$\n",
    "where\n",
    "$$\n",
    "B_j{}^l_m = R_j(r) Y^l_m.\n",
    "$$\n",
    "(We let origin be arbitrariy, or some \"important\" atom of the molecule for now.)\n",
    "The $c_{ij}{}^l_m$ for fixed $i, j, l$ transforms as an $l$-irrep.\n",
    "\n",
    "Radial basis $R(r)$ can be e.g. Bessel functions multiplied by an exponentially decaying envelope, or radial part of Slater-type orbitals, or radial part of Gaussian orbitals, or even a neural network. These choices may also involve a continuous parameter in the exponent that we optimize.\n",
    "\n",
    "The final wavefunction is the Slater determinant of molecular orbitals.\n",
    "\n",
    "Approach 2:\n",
    "Do Approach 1 around every atom, and sum up all wavefunctions.\n",
    "So we have a wavefunction $\\psi_A$ derived from Approach 1 around each atom, and sum everything together: $\\psi = \\sum_A \\psi_A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edxvCm_ARYyU",
   "metadata": {
    "id": "edxvCm_ARYyU"
   },
   "source": [
    "### Atoms with more electrons?\n",
    "\n",
    "Beyond helium, we'll have to antisymmetrize the wavefunction for electrons with the same spin. We'll explore this in the next few days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf9df02-d43f-46dc-94d9-6ee14786907e",
   "metadata": {
    "id": "bdf9df02-d43f-46dc-94d9-6ee14786907e",
    "tags": []
   },
   "source": [
    "# Molecules with multiple atoms\n",
    "\n",
    "If our $SO(3)$-equivariant neural network ansatz works well, we will try to extend our approach to molecules with multiple atoms. Exact details TBD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0X9xdWXE5kOV",
   "metadata": {
    "id": "0X9xdWXE5kOV"
   },
   "source": [
    "#Implementing Equivariance\n",
    "As a proof of concept, we will implement equivariance on the hydrogen atom. We take the total wavefunction to be the product of a radial part and an angular part, consisting of spherical harmonics which transform eqivariantly. The radial part as before will be invariant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IlitHwUG5jUm",
   "metadata": {
    "id": "IlitHwUG5jUm"
   },
   "outputs": [],
   "source": [
    "# Set up model\n",
    "\n",
    "radial = e3nn.flax.MultiLayerPerceptron([5, 1], act=jax.nn.gelu, output_activation=jax.nn.sigmoid)\n",
    "angular = e3nn.flax.MultiLayerPerceptron([5, 1], act=jax.nn.gelu, output_activation=jax.nn.sigmoid)\n",
    "N_samples = 2000\n",
    "max_r = 4\n",
    "regularize_pow = 8\n",
    "regularize_coeff = 100\n",
    "\n",
    "def wavefunction_H(w, in_points):\n",
    "  return mlp_H.apply(w, e3nn.tensor_square(e3nn.IrrepsArray(\"1o\", in_points)).filter(keep=\"0e\")).array.squeeze(-1)\n",
    "\n",
    "@jit\n",
    "def energy_H(w, in_points):\n",
    "  psi = wavefunction_H(w, in_points)\n",
    "  @vmap\n",
    "  def laplacian(in_points):\n",
    "    return jnp.einsum('ii->', jacobian(jacobian(wavefunction_H, argnums=1), argnums=1)(w, in_points))\n",
    "  laplacian_psi = laplacian(in_points)\n",
    "  cum_K = -0.5 * jnp.dot(psi, laplacian_psi)\n",
    "  distances = jnp.linalg.norm(in_points, axis=1)\n",
    "  cum_V = -jnp.dot(psi, psi / distances)\n",
    "  return (cum_K + cum_V) / jnp.dot(psi, psi)\n",
    "\n",
    "@jit\n",
    "def regularized_energy_H(w, in_points):\n",
    "  psi = wavefunction_H(w, in_points)\n",
    "  distances = jnp.linalg.norm(in_points, axis=1)\n",
    "  return energy_H(w, in_points) + regularize_coeff * jnp.dot(psi, psi * (distances / max_r) ** regularize_pow) / jnp.dot(psi, psi)  # penalize high probability near max_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ktnap6plq",
   "metadata": {
    "id": "283ktnap6plq"
   },
   "outputs": [],
   "source": [
    "random_key = jax.random.PRNGKey(0)\n",
    "in_points = jax.random.ball(random_key, 3, shape=(N_samples,)) * max_r\n",
    "random_key += 1\n",
    "x = e3nn.IrrepsArray(\"1o\", in_points)\n",
    "x = e3nn.tensor_square(x).filter(keep=\"0e\")\n",
    "\n",
    "w = mlp_H.init(random_key, x)\n",
    "random_key += 1\n",
    "%timeit print(jit(regularized_energy_H)(w, in_points))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hlDo5oO-p9uf",
   "metadata": {
    "id": "hlDo5oO-p9uf"
   },
   "source": [
    "#Metropolis Hastings\n",
    "This is a technique to sample directly from a wavefunction. Let $p(x) := |\\psi (x)|^2$ be the target distribution to sample from. Given a start point $x$, we generate a proposal point $x'$ from some proposal distribution $q(x; x')$, which could be, for example, a gaussian centered at $x'$. The proposal distribution is symmetric, that is to say $q(x;x') = q(x'; x)$. Then we calculate $a=\\min\\{1, p(x')/p(x)\\}$ and accept the proposal with probability $a$. This produces a random walk with stationary distribution that converges to $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HKkdcIUHp9Dv",
   "metadata": {
    "id": "HKkdcIUHp9Dv"
   },
   "outputs": [],
   "source": [
    "def energy_H(w, N_points = 10000):\n",
    "  psi = wavefunction_H(w, N_points)\n",
    "  @vmap\n",
    "  def laplacian(in_points):\n",
    "    return jnp.einsum('ii->', jacobian(jacobian(wavefunction_H, argnums=1), argnums=1)(w, in_points))\n",
    "  laplacian_psi = laplacian(in_points)\n",
    "  cum_K = -0.5 * jnp.dot(psi, laplacian_psi)\n",
    "  distances = jnp.linalg.norm(in_points, axis=1)\n",
    "  cum_V = -jnp.dot(psi, psi / distances)\n",
    "  return (cum_K + cum_V) / jnp.dot(psi, psi)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
